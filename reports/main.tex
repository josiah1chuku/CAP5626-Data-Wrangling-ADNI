\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{titlesec}

% Title Information
\title{ADNI Data Exploration, Wrangling, and Feature Engineering}
\author{Josiah Chuku \\ \small CAP5626 - Introduction to Artificial Intelligence \\ \small Florida A\&M University}
\date{February 12, 2026}

\begin{document}

\maketitle

\section{Introduction}
This report outlines the data preprocessing, cleaning, and feature engineering performed on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. The primary goal of this work is to prepare the \texttt{TADPOLE\_D3(in).csv} dataset (896 observations, 383 features) for subsequent machine learning tasks by addressing data quality issues and reducing dimensionality.

\section{Exploratory Data Analysis (EDA)}
Initial analysis revealed significant missingness in biomarker data and neuroimaging volumes. 
\begin{itemize}
    \item \textbf{Missing Data:} A heatmap was generated to visualize the sparsity of features. Brain volume measurements showed high rates of missingness, requiring an imputation strategy.
    \item \textbf{Target Distribution:} The distribution of diagnoses (DX) was analyzed to identify potential class imbalances.
\end{itemize}

\section{Data Cleaning and Preprocessing}
\subsection{Imputation and Outliers}
Biological data often contains noise and missing entries. The following decisions were made:
\begin{itemize}
    \item \textbf{Median Imputation:} Critical brain volumes were imputed using the median rather than the mean to remain robust against outliers.
    \item \textbf{Data Integrity:} Records with negative ages or impossible physiological volumes were flagged and treated as missing.
    \item \textbf{Feature Removal:} Administrative columns such as \texttt{RID} and \texttt{SITE} were dropped as they provide no predictive physiological value.
\end{itemize}

\subsection{Feature Engineering}
\begin{itemize}
    \item \textbf{Encoding:} Categorical variables (e.g., PTGENDER, DX) were transformed using One-Hot Encoding to allow for mathematical processing without implying an ordinal relationship.
    \item \textbf{Scaling:} Z-score standardization (\texttt{StandardScaler}) was applied to all numeric features. This ensures that features with large ranges, such as \texttt{WholeBrain} volume, do not disproportionately influence the model compared to smaller-scale features like \texttt{Age}.
\end{itemize}

\section{Dimensionality Reduction}
To address the \textit{Curse of Dimensionality}, Principal Component Analysis (PCA) was implemented.
\begin{itemize}
    \item \textbf{Variance Capture:} The high-dimensional feature space (383 features) was reduced to 2 principal components.
    \item \textbf{Visualization:} The resulting 2D plot illustrated emerging clusters corresponding to different diagnostic groups, demonstrating that significant variance is captured even in a reduced space.
\end{itemize}

\section{Conclusion}
The preprocessing pipeline successfully transformed raw ADNI data into a clean, scaled feature matrix. By addressing missingness through median imputation and mitigating redundancy via PCA, the dataset is now optimized for classification models. Future work will explore longitudinal trends across study phases to improve predictive accuracy.

\end{document}
